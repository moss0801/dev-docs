
### 3.4 카프카 클라이언트

프로듀서, 컨슈머, 어드민

#### 3.4.1 프로듀서 API

프로듀서는 데이터의 시작점 +
카프카에 필요한 데이터를 선언하고 브로커의 특정 토픽의 파티션에 전송 +
데이터를 전송할 때 리더 파티션을 가지고 있는 카프카 브로커와 직접 통신

직렬화: 자바 또는 외부 시스템에서 사용 가능하도록 바이트 형태로 데이터를 변환하는 기술

[NOTE]
====
```
java, gradle 설치 - 바이너리 다운받음 후, 압축 해제 후, 각 폴더의 bin 경로를 PATH에 추가

$ java -version
openjdk version "17.0.2" 2022-01-18
OpenJDK Runtime Environment (build 17.0.2+8-Ubuntu-120.04)
OpenJDK 64-Bit Server VM (build 17.0.2+8-Ubuntu-120.04, mixed mode, sharing)

$ gradle -v

Welcome to Gradle 7.4.1!

Here are the highlights of this release:
 - Aggregated test and JaCoCo reports
 - Marking additional test source directories as tests in IntelliJ
 - Support for Adoptium JDKs in Java toolchains

For more details see https://docs.gradle.org/7.4.1/release-notes.html


------------------------------------------------------------
Gradle 7.4.1
------------------------------------------------------------

Build time:   2022-03-09 15:04:47 UTC
Revision:     36dc52588e09b4b72f2010bc07599e0ee0434e2e

Kotlin:       1.5.31
Groovy:       3.0.9
Ant:          Apache Ant(TM) version 1.10.11 compiled on July 10 2021
JVM:          17.0.2 (Private Build 17.0.2+8-Ubuntu-120.04)
OS:           Linux 5.10.16.3-microsoft-standard-WSL2 amd64
```
====

**카프카 프로듀서 프로젝트 생성**
gradle를 이용하여 프로젝트 초기화

```
gradle init --type java-application --dsl groovy --test-framework junit-jupiter
mv app/* .
rmdir app
```

```
$ gradle init --type java-application --dsl groovy --test-framework junit-jupiter

Generate build using new APIs and behavior (some features may change in the next minor release)? (default: no) [yes, no]
 no

Project name (default: kafka-producer): kafka-producer
Source package (default: kafka.producer): com.moss.kafka.producer

> Task :init
Get more help with your project: https://docs.gradle.org/7.4.1/samples/sample_building_java_applications.html

BUILD SUCCESSFUL in 23s
2 actionable tasks: 2 executed
```

초기화 후 폴더 구조
```
.
├── build.gradle       # 1
├── gradle             # 2
│   └── wrapper
│       ├── gradle-wrapper.jar
│       └── gradle-wrapper.properties
├── gradlew
├── gradlew.bat
├── settings.gradle
└── src
    ├── main
    │   ├── java       # 3
    │   │   └── com
    │   │       └── moss
    │   │           └── kafka
    │   │               └── producer
    │   │                   └── App.java
    │   └── resources
    └── test
        ├── java
        │   └── com
        │       └── moss
        │           └── kafka
        │               └── producer
        │                   └── AppTest.java
        └── resources
```

. 프로젝트를 빌드하기 위한 작업이나 디펜더시를 정의 가능
. 그래들(gradle)로 생성한 프로젝트의 구조를 선언. 주로 멀티 프로젝트 구조 정의
. 코드가 위치할 디렉토리

**settings.gradle, build.gradle 수정**

link:https://docs.confluent.io/platform/current/clients/index.html[Kafka Clients]

.settings.gradle
```
rootProject.name = 'kafka-producer'
```

.build.gradle
```
plugins {
    id 'java'
    id 'application'
}

group 'com.moss.kafka'
version '1.0'

repositories {
    mavenCentral()
}

dependencies {
    implementation 'org.apache.kafka:kafka-clients:3.1.0'
    implementation 'org.slf4j:slf4j-simple:1.7.36'

    testImplementation 'org.junit.jupiter:junit-jupiter:5.8.1'
}

application {
    mainClass = 'com.moss.kafka.producer.App'
}

tasks.named('test') {
    useJUnitPlatform()
}
```

**src/main/java/com/moss/kafka/producer/App.java 수정**

.App.java
```
package com.moss.kafka.producer;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.serialization.StringSerializer;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.Properties;

public class App {
    private final static Logger logger = LoggerFactory.getLogger(App.class);
    private final static String TOPIC_NAME = "test";  // 1
    private final static String BOOTSTRAP_SERVERS = "localhost:9092,localhost:9093,localhost:9094";  // 2

    public static void main(String[] args) {
        Properties configs = new Properties();  // 3
        configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS);
        configs.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());  // 4
        configs.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());  // 4

        KafkaProducer<String, String> producer = new KafkaProducer<>(configs);  // 5

        String messageValue = "testMessage";  // 6
        ProducerRecord<String, String> record = new ProducerRecord<>(TOPIC_NAME, messageValue);  /7
        producer.send(record);  // 8
        logger.info("{}", record);
        producer.flush();  // 9
        producer.close();  // 10
    }
}
```

설명 추가
```
package com.moss.kafka.producer;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.serialization.StringSerializer;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.Properties;

public class App {
    private final static Logger logger = LoggerFactory.getLogger(App.class);
    // 1. 토픽 이름
    private final static String TOPIC_NAME = "test";
    // 2. 카프카 클러스터 서버 IP:port 목록
    private final static String BOOTSTRAP_SERVERS = "localhost:9092,localhost:9093,localhost:9094";

    public static void main(String[] args) {
        // 3. KafkaProducer 옵션
        // 필수 옵션은 반드시 선언, 선택 옵션은 선택하지 않으면 기본 옵션값으로 동작, 선택 옵션의 기본 값을 알아야 한다.
        Properties configs = new Properties();
        configs.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS);
        // 4. 메시지 키, 메시지 값 직렬화 클래스 선언
        configs.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
        configs.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());

        // 5. Properties와 함께 KafkaProducer 인스턴스 생성. producer 인스턴스는 ProducerRecord 전송 시 사용.
        KafkaProducer<String, String> producer = new KafkaProducer<>(configs);

        // 6. 메시지 값 선언
        String messageValue = "testMessage";
        // 7. 브로커로 데이터를 보내기 위해 ProducerRecord 생성.
        // ProducerRecord 생성자는 여러 개. 생성자 개수에 따라 오버로딩(overloading) 되어 생성
        // 여기서는 토픽 이름과 메시지 값만 선언. 메시지 키는 null로 설정되어 전송
        // ProducerRecord의 2개의 제네릭 값은 각각 메시지 키와 메시지 값 타입을 의미.
        // 메시지 키와 메시지 값의 타입은 직렬화 클래스와 동일하게 설정
        ProducerRecord<String, String> record = new ProducerRecord<>(TOPIC_NAME, messageValue);
        // 8. recode를 producer의 send() 메서드로 전송
        // 프로듀서에서 send()는 즉각 전송하지 않고, record를 프로듀서 내부에 가지고 있다가 배치 형태로 브로커에 전송.
        // 이러한 전송 방식을 '배치 전송'이라 부름. 배치 전송을 통해 카프카는 타 메시지 플랫폼과 차별화된 전송 속도를 가짐.
        producer.send(record);
        logger.info("{}", record);
        // 9. flush()를 통해 프로듀서 내부 버퍼에 가지고 있던 레코드 배치를 브로커로 즉각 전송
        producer.flush();
        // 10. 애플리케이션 종료하기 전에 close() 메서드를 호출하여 producer 인스턴스의 리소스들을 안전하게 종료.
        producer.close();
    }
}
```

실제 운영환경에서는 추가 로직들을 포함.

* 프로듀서 인스턴스를 통해 데이터가 카프카 보러커까지 정상적으로 전송되었는지 확인하는 로직
* 환경에 따른 추가적인 프로듀서 선택 옵션들의 설정 등

**test 토픽 생성**

test 토픽이 생성되었는지 확인하고 생성되지 않았으면 생성한다.

.토픽 목록 확인
```
$ bin/kafka-topics.sh --list --bootstrap-server localhost:9092,localhost:9093,localhost:9094
__consumer_offsets
test
```

.토필 생성 : test 토픽이 없는 경우
```
$ bin/kafka-topics.sh --bootstrap-server localhost:9092,localhost:9093,localhost:9094 \
  --create \
  --topic test \
  --partitions 3
Created topic test.
```

**프로그램 실행**
'./gradlew run' 명령어로 실행

```
$ ./gradlew run

> Task :run
#### 1 ####
[main] INFO org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values:
        acks = -1
        batch.size = 16384
        bootstrap.servers = [localhost:9092, localhost:9093, localhost:9094]
        buffer.memory = 33554432
        client.dns.lookup = use_all_dns_ips
        client.id = producer-1
        compression.type = none
        connections.max.idle.ms = 540000
        delivery.timeout.ms = 120000
        enable.idempotence = true
        interceptor.classes = []
        key.serializer = class org.apache.kafka.common.serialization.StringSerializer
        linger.ms = 0
        max.block.ms = 60000
        max.in.flight.requests.per.connection = 5
        max.request.size = 1048576
        metadata.max.age.ms = 300000
        metadata.max.idle.ms = 300000
        metric.reporters = []
        metrics.num.samples = 2
        metrics.recording.level = INFO
        metrics.sample.window.ms = 30000
        partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
        receive.buffer.bytes = 32768
        reconnect.backoff.max.ms = 1000
        reconnect.backoff.ms = 50
        request.timeout.ms = 30000
        retries = 2147483647
        retry.backoff.ms = 100
        sasl.client.callback.handler.class = null
        sasl.jaas.config = null
        sasl.kerberos.kinit.cmd = /usr/bin/kinit
        sasl.kerberos.min.time.before.relogin = 60000
        sasl.kerberos.service.name = null
        sasl.kerberos.ticket.renew.jitter = 0.05
        sasl.kerberos.ticket.renew.window.factor = 0.8
        sasl.login.callback.handler.class = null
        sasl.login.class = null
        sasl.login.connect.timeout.ms = null
        sasl.login.read.timeout.ms = null
        sasl.login.refresh.buffer.seconds = 300
        sasl.login.refresh.min.period.seconds = 60
        sasl.login.refresh.window.factor = 0.8
        sasl.login.refresh.window.jitter = 0.05
        sasl.login.retry.backoff.max.ms = 10000
        sasl.login.retry.backoff.ms = 100
        sasl.mechanism = GSSAPI
        sasl.oauthbearer.clock.skew.seconds = 30
        sasl.oauthbearer.expected.audience = null
        sasl.oauthbearer.expected.issuer = null
        sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
        sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
        sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
        sasl.oauthbearer.jwks.endpoint.url = null
        sasl.oauthbearer.scope.claim.name = scope
        sasl.oauthbearer.sub.claim.name = sub
        sasl.oauthbearer.token.endpoint.url = null
        security.protocol = PLAINTEXT
        security.providers = null
        send.buffer.bytes = 131072
        socket.connection.setup.timeout.max.ms = 30000
        socket.connection.setup.timeout.ms = 10000
        ssl.cipher.suites = null
        ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
        ssl.endpoint.identification.algorithm = https
        ssl.engine.factory.class = null
        ssl.key.password = null
        ssl.keymanager.algorithm = SunX509
        ssl.keystore.certificate.chain = null
        ssl.keystore.key = null
        ssl.keystore.location = null
        ssl.keystore.password = null
        ssl.keystore.type = JKS
        ssl.protocol = TLSv1.3
        ssl.provider = null
        ssl.secure.random.implementation = null
        ssl.trustmanager.algorithm = PKIX
        ssl.truststore.certificates = null
        ssl.truststore.location = null
        ssl.truststore.password = null
        ssl.truststore.type = JKS
        transaction.timeout.ms = 60000
        transactional.id = null
        value.serializer = class org.apache.kafka.common.serialization.StringSerializer

#### 2 ####
[main] INFO org.apache.kafka.common.utils.AppInfoParser - Kafka version: 3.1.0
[main] INFO org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 37edeed0777bacb3
[main] INFO org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1648054717171
[kafka-producer-network-thread | producer-1] INFO org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Resetting the last seen epoch of partition test-0 to 21 since the associated topicId changed from null to n3nYAgNIQ8aIcxrMbcn24A
[kafka-producer-network-thread | producer-1] INFO org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Resetting the last seen epoch of partition test-2 to 16 since the associated topicId changed from null to n3nYAgNIQ8aIcxrMbcn24A
[kafka-producer-network-thread | producer-1] INFO org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Resetting the last seen epoch of partition test-3 to 21 since the associated topicId changed from null to n3nYAgNIQ8aIcxrMbcn24A
[kafka-producer-network-thread | producer-1] INFO org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Resetting the last seen epoch of partition test-1 to 16 since the associated topicId changed from null to n3nYAgNIQ8aIcxrMbcn24A
[kafka-producer-network-thread | producer-1] INFO org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: 4qcm1av-QEm2eFZVApehnw

#### 3 ####
[main] INFO com.moss.kafka.producer.App - ProducerRecord(topic=test, partition=null, headers=RecordHeaders(headers = [], isReadOnly = true), key=null, value=testMessage, timestamp=null)
[main] INFO org.apache.kafka.clients.producer.KafkaProducer - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms.
[main] INFO org.apache.kafka.common.metrics.Metrics - Metrics scheduler closed
[main] INFO org.apache.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[main] INFO org.apache.kafka.common.metrics.Metrics - Metrics reporters closed
[main] INFO org.apache.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-1 unregistered

BUILD SUCCESSFUL in 5m 59s
2 actionable tasks: 2 executed
```

. ProducerConfig values : 카프카 프로듀서 구동 시 설정한 옵션
. Kafka version: 3.1.0 : 카프카 버전
. ProducerRecord(topic=test, partition=null, headers=RecordHeaders(headers = [], isReadOnly = true), key=null, value=testMessage, timestamp=null) +
전송한 ProducerRecord. 생성 시 메시지 키를 설정하지 않았기에 null로 설정된 것 확인.

.kafka-console-consumer 로 데이터 확인
```
$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092,localhost:9093,localhost:9094 \
  --topic test \
  --from-beginning
...
testMessage
```

**프로듀서 중요 개념**

.프로듀서 애플리케이션은 카프카 클러스터로 레코드를 전송한다
image:imgs/producer application.svg[Static, 800]

프로듀서는 카프카 브로커로 데이터를 전송할 때 내부적으로 파티셔너, 배치 생성 단계를 거침. +
ProducerRecord 인스턴스는 필수 파라미터인 토픽과 메시지 값만 설정 +
아래의 값을 추가로 설정할 수 있다.

* 파티션 번호
* 타임스탬프
* 메시지 키

.ProducerRecord 생성자 목록
```
public ProducerRecord(String topic, Integer partition, Long timestamp, K key, V value, Iterable<Header> headers)
public ProducerRecord(String topic, Integer partition, Long timestamp, K key, V value)
public ProducerRecord(String topic, Integer partition, K key, V value, Iterable<Header> headers)
public ProducerRecord(String topic, Integer partition, K key, V value)
public ProducerRecord(String topic, K key, V value)
public ProducerRecord(String topic, V value)
```

KafkaProducer.send() 메서드를 호출하면 ProducerRecord는 파티셔너(partitioner)에서 토픽의 어느 파티션으로 전송될 것인지 정해짐.
KafkaProducer 생성 시 파티셔너를 지정하지 않으면 DefaultPartitoner을 설정.

파티셔너에 의해 구분된 레코드는 데이터를 전송하기 전에 어큐뮬레이터(accumulator)에 데이터를 버퍼로 쌓아놓고 발송. +
버퍼로 쌓인 데이터는 배치로 묶어서 전송함으로써 카프카의 프로듀서 처리량을 향상시키는 데에 상당한 도움을 줌.

Partitioner (3.1.0 버전 기준)

* DefaultPartitioner (Default) link:https://github.com/a0x8o/kafka/blob/master/clients/src/main/java/org/apache/kafka/clients/producer/internals/DefaultPartitioner.java[SourceCode] : +
Key 존재 시 Utils.murmur2() 해쉬 값 사용, Key 미존재 시 StickyPartitionCache 사용
* UniformStickyPartitioner link:https://github.com/a0x8o/kafka/blob/master/clients/src/main/java/org/apache/kafka/clients/producer/UniformStickyPartitioner.java[SourceCode] :
항상 StickyPartitionCache 사용
* RoundRobinPartitioner link:https://github.com/a0x8o/kafka/blob/master/clients/src/main/java/org/apache/kafka/clients/producer/RoundRobinPartitioner.java[SourceCode] :
항상 RoundRobin

link:https://www.confluent.io/blog/apache-kafka-producer-improvements-sticky-partitioner/[Apache Kafka Producer Improvements with the Sticky Partitioner]
image:imgs/sticky-partitioner-strategy.png[]

RoundRobinPartitioner는 같은 Topic의 동일 Batch 내 레코드여도 매번 다른 파티션이 선택하지만, +
StickyPartitioner는 같은 Topic의 동일 batch 내 레코드면 같은 파티션을 선택. +
다음 배치에는 이전 파티션과 아닌 파티션을 랜덤하게 선택.

```
public class StickyPartitionCache {
    private final ConcurrentMap<String, Integer> indexCache;
    public StickyPartitionCache() {
        this.indexCache = new ConcurrentHashMap<>();
    }

    public int partition(String topic, Cluster cluster) {
        Integer part = indexCache.get(topic);
        if (part == null) {
            return nextPartition(topic, cluster, -1);
        }
        return part;
    }

    public int nextPartition(String topic, Cluster cluster, int prevPartition) {
        List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);
        Integer oldPart = indexCache.get(topic);
        Integer newPart = oldPart;
        // Check that the current sticky partition for the topic is either not set or that the partition that
        // triggered the new batch matches the sticky partition that needs to be changed.
        if (oldPart == null || oldPart == prevPartition) {
            List<PartitionInfo> availablePartitions = cluster.availablePartitionsForTopic(topic);
            if (availablePartitions.size() < 1) {
                Integer random = Utils.toPositive(ThreadLocalRandom.current().nextInt());
                newPart = random % partitions.size();
            } else if (availablePartitions.size() == 1) {
                newPart = availablePartitions.get(0).partition();
            } else {
                while (newPart == null || newPart.equals(oldPart)) {
                    int random = Utils.toPositive(ThreadLocalRandom.current().nextInt());
                    newPart = availablePartitions.get(random % availablePartitions.size()).partition();
                }
            }
            // Only change the sticky partition if it is null or prevPartition matches the current sticky partition.
            if (oldPart == null) {
                indexCache.putIfAbsent(topic, newPart);
            } else {
                indexCache.replace(topic, prevPartition, newPart);
            }
            return indexCache.get(topic);
        }
        return indexCache.get(topic);
    }
}
```

사용자 지정 파티셔너를 생성하기 위해서는 Partitioner 인터페이스를 구현.

파티셔너를 통해 파티션이 지정된 데이터는 어큐뮬레이터에 버퍼로 쌓임. +
센더(sender) 스레드는 어큐뮬레이터에 쌓인 배치 데이터를 가져가 카프카 브로커로 전송.

브로커로 전송 시 압축 방식을 정할 수 있음. +
압축 옵션을 정하지 않으면 압축이 되지 않은 상태로 전송 +
압축 옵션: gzip, snappy, lz4, zstd

압축을 하면 데이터 전송 시 네트워크 처리량에 이득을 볼 수 있지만 압축을 하는 데에 CPU 또는 메모리 리소스를 사용하므로 사용환경에 따라 적절한 압축 옵션을 사용하는 것이 중요.
또한, 프로듀서에서 압축한 메시지는 컨슈머 애플리케이션이 압축을 풀게 되는데 이때도 컨슈머 애플리케이션 리소스가 사용되는 점을 주의.

[NOTE]
====
압축 참고: link:http://happinessoncode.com/2019/01/18/kafka-compression-ratio/[Kafka 메시지 압축률 안나와서 고생한 이야기]

* 카프카를 사용하고자 한다면 압축을 고려해보자.
* 생각보다 압축률이 나오지 않는다면 linger.ms와 batch.size를 조절해보자.

.프로듀서
|===
|type|CPU 사용률|처리 시간|브로커 디스크 용량|압축률

|none|1.00|1.00|1.00|1.00
|gzip|1.70|0.84|0.31|3.17
|lz4|1.54|0.61|0.46|2.16
|snappy|1.54|0.60|0.46|2.16
|===

.컨슈머
|===
|type|CPU 사용률|처리시간

|none|1.00|1.00
|gzip|1.54|0.65
|lz4|1.35|0.68
|snappy|1.35|0.72
|===

====

**프로듀서 주요 옵션**

link:https://kafka.apache.org/documentation.html#producerconfigs[Producer Configs] +
link:https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/ProducerConfig.java#L316[Default Cofig]

[cols="1,9"]
.필수옵션
|===
|옵션|설명

|bootstrap.servers|브로커 호스트 이름:포트 1개 이상. 2개 이상 입력하면 일부 브로커에 이슈가 발생하더라도 접속하는데 이슈 없도록 설정 가능.
|key.serializer|레코드의 메시지 키를 직렬화하는 클래스
|value.serializer|레코드의 메시지 값을 직렬화하는 클래스
|===

[cols="1,9"]
.선택옵션
|===
|옵션|설명

|akcs
a|기본값: all +
프로듀서가 전송한 데이터가 브로커들에 정상적으로 저장되었는지 전송 성공 여부 확인하는 데 사용하는 옵션. 0, 1, -1(all) 중 하나서 설정 가능. +
ㆍ 1 (default): 리더 파티션에 데이터가 저장되면 전송 성공으로 판단 +
ㆍ 0 : 프로듀서가 전송한 즉시 브로커에 데이터 저장 여부와 상관 없이 성공으로 판단 +
ㆍ -1 (or all): 토픽의 min.insync.replicas 개수에 해당하는 리더 파티션과 팔로워 파티션에 데이터가 저장되면 성공으로 판단

|buffer.memory|기본값: 33554432(32MB) +
브로커로 전송할 데이터를 배치에 모으기 위해 설정한 버퍼 메모리량
|retries|기본값: 2147483647 +
프로듀서가 브로커로부터 에러를 받고 난 뒤 재전송을 시도하는 횟수 +
|**batch.size**|기본값: 16384 (16kB) +
배치로 전송할 레코드 최대 용량 +
너무 작게 설정하면 프로듀서가 브로커로 더 자주 보내기 때문에 네트워크 부담 +
너무 크게 설정하면 메모리를 더 많이 사용 +
[개인학습] batch.size는 partition 별로 관리된다. +
```
Requests sent to brokers will contain multiple batches, one for each partition with data available to be sent.
```
|**linger.ms**|기본값: 0 +
배치를 전송하기 전까지 기다리는 최소 시간 +
[개인학습] 프로듀서는 메시지 도착 속도가 메시지 전송 속도보다 빠를 때만 배치로 묶어서 보냄 +
0인 경우 메시지를 바로바로 브로커로 전송할 수 있는 상황에서는 배치가 쌓일 때까지 기다리지 않고 전송 +
기본값이 0이기 때문에 전송 가능하다면 바로 전송되어 레코드들이 배치로 묶이지 않음. +
batch 효율을 높이기 위한 문서상 예제 값은 5 (5ms) +
```
The producer groups together any records that arrive in between request transmissions into a single batched request. Normally this occurs only under load when records arrive faster than they can be sent out.
```
|partitioner.class|기본값: org.apache.kafka.clients.producer.internals.DefaultPartitioner +
레코드를 파티션에 전송할 때 적용하는 파티셔너 클래스
|enable.idempotence
a|기본값: true +
멱등성 프로듀서로 동작 여부 +
[개인학습] true인 경우 필요 조건 +
ㆍ retries : value > 0 (기본값: Integer.MAX_VALUE) +
ㆍ acks : all (기본값: all)
|transactional.id|기본값: null +
프로듀서가 레코드를 전송할 때 레코드를 트랜잭션 단위로 묶을지 여부 설정 +
프로듀서의 고유한 트랜잭션 아이디를 설정할 수 있다. 이 값을 설정하면 트랜잭션 프로듀서로 동작 +
자세한 내용은 챕터 4에서 확인
|compression.type
a|기본값: none +
데이터 압축 유형, 사용가능 값은 (none, gzip, snappy, lz4, zstd) +
압축은 배치 단위로 이루어 짐. 이에 효율적인 배치는 효율적 압축률로 이어짐.

연관 주요 설정 +
ㆍ batch.size +
ㆍ linger.ms : 5

link:https://developer.ibm.com/articles/benefits-compression-kafka-messaging/[Message compression in Apache Kafka]

[개인정리] 실시간 데이터(이벤트 등)는 snappy, 아카이브 데이터(로그 등)는 gzip 사용이 유리할 것으로 생각 +
snappy는 반복적인 패턴이 존재하는 경우 효율적. json은 반복적인 패턴이 많기에 snappy가 효율적

|===

**압축 유형(compresssion.type) 비교**

|===
|Compression type|Compression ratio|CPU usage|Compression speed|Network bandwidth usage

|Gzip|Highest|Highest|Slowest|Lowest
|Snappy|Medium|Moderate|Moderate|Medium
|Lz4|Low|Lowest|Fastest|Highest
|Zstd|Medium|Moderate|Moderate|Medium
|===

**IBM 압축 유형별 테스트 결과**

* data: 1000 messages in JSON format with an average size of 10 KB
* total data: 10 MB
* serializer: org.apache.kafka.common.serialization.StringSerialize

|===
|Metrics|Uncompressed|Gzip|Snappy|lz4|Zstd

|Avg latency (ms)|65|10.41|10.1|9.26|10.78
|Disk space (mb)|10|0.92|2.18|2.83|1.47
|Effective compression ratio|1|0.09|0.21|0.28|0.15
|Process CPU usage %|2.35|11.46|7.25|5.89|8.93
|===

정리: 실시간 데이터(이벤트 등)는 snappy, 아카이브 데이터(로그 등)는 gzip

의문: 데이터 크기가 아주 작은 경우에는 Uncompressed가 가장 빠를까?

**메시지 키를 가진 데이터를 전송하는 프로듀서**

.메시지 키 포함된 레코드 생성
```
// Topic, Key, Value
ProducerRecord<String, String> record = new ProducerRecord<>("test", "Pangyo", "23");
```

.레코드 확인
```
$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092,localhost:9093,localhost:9094 \
  --topic test \
  --property print.key=true \
  --property key.separator="-" \
  --from-beginning
...
null-testMessage
Pangyo-23
```

.동일한 메시지 키는 동일한 파티션에 지정되어 적재된다. (DefaultPartitioner)
image:imgs/same key  to same partition.svg[Static, 800]

.파티션 지정
```
// Topic, PartitionNo, Key, Value
ProducerRecord<String, String> record = new ProducerRecord<>("test", 1, "Pangyo", "23");
```

**커스텀 파티셔너를 가지는 프로듀서**

특정 데이터를 가지는 레코드는 특정 파티션으로 보내야 하는 경우 존재 +
기본 파티셔너를 사용할 경우 메시지 키의 해시값을 파티션에 매칭하여 데이터를 전송하므로 어느 파티션에 들어가는지 알 수 없음

Partitioner 인터페이스를 사용하여 사용자 정의 파티셔너를 생성하면 Pangyo라는 값을 가진 메시지 키에 대해서 무조건 파티션 1번으로 지정하도록 설정 가능 +
이 경우 토픽의 파티션 개수가 변경되더라도, Pangyo라는 메시지 키를 가진 데이터를 파티션 1번에 적재

```
package com.moss.kafka.producer;

import org.apache.kafka.clients.producer.Partitioner;
import org.apache.kafka.common.Cluster;
import org.apache.kafka.common.InvalidRecordException;
import org.apache.kafka.common.PartitionInfo;
import org.apache.kafka.common.utils.Utils;

import java.util.List;
import java.util.Map;

public class CustomPartitioner implements Partitioner {
    // # 1
    @Override
    public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {
        // # 2
        if (keyBytes == null) {
            throw new InvalidRecordException("Need message key");
        }
        // # 3
        if (((String)key).equals("Pangyo")) {
            return 1;
        }

        List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);
        int numPartitions = partitions.size();
        // # 4
        return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;
    }

    @Override
    public void close() { }

    @Override
    public void configure(Map<String, ?> configs) { }
}
```

. partition 메서드에는 레코드를 기반으로 파티션을 정하는 로직을 포함. 반환값은 주어진 레코드가 들어간 파티션 번호
. 레코드에 메시지 키를 지정하지 않은 경우에는 비정상적인 데이터로 간주하고 InvalidRecordException 발생
. 메시지 키가 Pangyo인 경우 파티션 1번으로 지정되도록 1을 반환
. Pangyo가 아닌 메시지 키를 가진 레코드는 해시값을 지정하여 특정 파티션에 매칭되도록 설정

.CustomPartitioner를 설정하여 KafkaProducer 생성
```
Properties configs = new Properties();
...
configs.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, CustomPartitioner.class);

KafkaProducer<String, String> producer = new KafkaProducer<>(configs);
```

**브로커 정상 전송 여부를 확인하는 프로듀서**

KafkaProducer의 send() 메서드는 Future 객체를 반환 +
이 객체는 RecordMetadata의 비동기 결과를 표현, ProducerRecord가 카프카 브로커에 정상적으로 적재되었는지에 대한 데이터를 포함 +
get() 메서드를 사용하면 프로듀서로 보낸 데이터의 결과를 동기적으로 조회 가능

```
ProducerRecord<String, String> record = new ProducerRecord<>(TOPIC_NAME, messageValue);
RecordMetadata metadata = producer.send(record).get();
logger.info(metadata.toString());
```

send()의 결과값은 카프라 브로커로부터 응답을 기다렸다가 브로커로부터 응답이 오면 RecordMetadata 인스턴스를 반환

.Metadata 출력화면, {topic}-{partition}@{offset}
```
[main] INFO com.moss.kafka.producer.App - test-1@11
```

test-1@11

* 토픽 이름: test
* 파티션 번호: 1
* 오프셋 번호: 11

동기로 프로듀서의 전송 결과를 확인하는 것은 빠른 전송에 허들 될 수 있음 +
프로듀서가 전송하고 난 뒤 브로커로부터 전송에 대한 응답 값을 받기 전까지 대기하기 때문

비동기로 결과 확인을 위한 Callback 인터페이스 제공

```
package com.moss.kafka.producer;

import org.apache.kafka.clients.producer.Callback;
import org.apache.kafka.clients.producer.RecordMetadata;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class ProducerCallback implements Callback {
    private final static Logger logger = LoggerFactory.getLogger(ProducerCallback.class);

    @Override
    public void onCompletion(RecordMetadata metadata, Exception exception) {
        if (exception != null) {
            logger.error(exception.getMessage(), exception);
        } else {
            logger.info(metadata.toString());
        }
    }
}
```

onCompletion 메서드는 레코드의 비동기 결과를 받기 위해 사용 +
브로커 적재에 이슈가 생겼을 경우 Exception 에 어떤 에러가 발생하였는지 담겨서 메서드가 실행 +
에러가 발생하지 않은 경우 RecordMetadata 를 통해 해당 레코드가 적재된 토픽 이름과 파티션 번호, 오프셋을 확인

.ProducerCallback과 함께 send()
```
ProducerRecord<String, String> record = new ProducerRecord<>(TOPIC_NAME, messageValue);
ProducerCallback callback = new ProducerCallback();
producer.send(record, callback);
```

.ProducerCallback 실행 결과
```
[kafka-producer-network-thread | producer-1] INFO com.moss.kafka.producer.ProducerCallback - test-3@5
```

비동기로 결과를 받을 경우 동기로 결과를 받는 경우보다 더 빠른 속도로 데이터를 추가 처리 가능 +
하지만 전송하는 데이터의 순서가 중요한 경우 사용하면 안됨 +
비동기로 결과를 기다리는 동안 다음으로 보낼 데이터의 전송이 성공하고 앞서 보낸 데이터의 결과가 실패할 경우 재전송으로 인해 데이터 순서가 역전될 수 있음 +
데이터의 순서가 중요하다면 동기로 전송 결과를 받아야 함
