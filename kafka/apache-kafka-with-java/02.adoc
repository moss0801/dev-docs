
## 2 카프카 빠르게 시작해보기

### 2.1 실습용 카프카 브로커 설치

*OS*

WSL2 Ubuntu 20.04

#### 자바 설치

*Java 설치*

Kafka, Zookeeper 는 Java 1.8 이상 버전이 필요 +
최신 LTS 버전인 17을 설치해보기
```
sudo apt-get update
sudo apt-get upgrade
sudo apt-get install openjdk-17-jdk
```

Java 설치 확인
```
$ java -version
openjdk version "17.0.2" 2022-01-18
OpenJDK Runtime Environment (build 17.0.2+8-Ubuntu-120.04)
OpenJDK 64-Bit Server VM (build 17.0.2+8-Ubuntu-120.04, mixed mode, sharing)
```

#### 주키퍼, 카프카 브로커 실행

*Kafka Broker 설치*

Kafka Quickstart : https://kafka.apache.org/quickstart +
Download : https://www.apache.org/dyn/closer.cgi?path=/kafka/3.1.0/kafka_2.13-3.1.0.tgz

```
wget https://dlcdn.apache.org/kafka/3.1.0/kafka_2.13-3.1.0.tgz
tar xvf kafka_2.13-3.1.0.tgz
cd kafka_2.13-3.1.0
```

Kafka Broker Heap Memory 설정 +
재실행 시에도 적용되도록 .bashrc에 추가
```
export KAFKA_HEAP_OPTS="-Xmx400m -Xms400m"
echo $KAFKA_HEAP_OPTS
echo 'export KAFKA_HEAP_OPTS="'$KAFKA_HEAP_OPTS'"' | tee -a $HOME/.bashrc
```

$HOME/.bashrc 확인
```
$ cat $HOME/.bashrc
...
export KAFKA_HEAP_OPTS="-Xmx400m -Xms400m"
```

Kafka Broker Heap Memory 기본값: -Xmx1G -Xms1G +
bin/kafka-server-start.sh
```
...
if [ "x$KAFKA_HEAP_OPTS" = "x" ]; then
    export KAFKA_HEAP_OPTS="-Xmx1G -Xms1G"
fi
...
```

*카프카 브로커 실행 옵션 설정*

config/server.properties 에서 카프카 브로커가 클러스터 운영에 필요한 옵션들을 지정할 수 있다. +
실습 시에는 카프카 클라이언트 또는 커맨드 라인 툴을 브로커와 연결하기 위한 advertised.listeners 만 설정.

```
#advertised.listeners=PLAINTEXT://your.host.name:9092
```

위 설정을 아래와 같이 주석을 푼 뒤 localhost(127.0.0.1)로 수정

```
advertised.listeners=PLAINTEXT://127.0.0.1:9092
```


config/server.properties
```
...
############################# Server Basics #############################

# The id of the broker. This must be set to a unique integer for each broker.
##### 1 #####
broker.id=0

############################# Socket Server Settings #############################

# The address the socket server listens on. It will get the value returned from
# java.net.InetAddress.getCanonicalHostName() if not configured.
#   FORMAT:
#     listeners = listener_name://host_name:port
#   EXAMPLE:
#     listeners = PLAINTEXT://your.host.name:9092
##### 2 #####
#listeners=PLAINTEXT://:9092

# Hostname and port the broker will advertise to producers and consumers. If not set,
# it uses the value for "listeners" if configured.  Otherwise, it will use the value
# returned from java.net.InetAddress.getCanonicalHostName().
##### 3 #####
advertised.listeners=PLAINTEXT://127.0.0.1:9092

# Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details
##### 4 #####
#listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL

# The number of threads that the server uses for receiving requests from the network and sending responses to the network
##### 5 #####
num.network.threads=3

# The number of threads that the server uses for processing requests, which may include disk I/O
##### 6 #####
num.io.threads=8

# The send buffer (SO_SNDBUF) used by the socket server
socket.send.buffer.bytes=102400

# The receive buffer (SO_RCVBUF) used by the socket server
socket.receive.buffer.bytes=102400

# The maximum size of a request that the socket server will accept (protection against OOM)
socket.request.max.bytes=104857600


############################# Log Basics #############################

# A comma separated list of directories under which to store log files
##### 7 #####
log.dirs=/tmp/kafka-logs

# The default number of log partitions per topic. More partitions allow greater
# parallelism for consumption, but this will also result in more files across
# the brokers.
##### 8 #####
num.partitions=1

# The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.
# This value is recommended to be increased for installations with data dirs located in RAID array.
num.recovery.threads.per.data.dir=1

############################# Internal Topic Settings  #############################
# The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state"
# For anything other than development testing, a value greater than 1 is recommended to ensure availability such as 3.
offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1

############################# Log Flush Policy #############################

# Messages are immediately written to the filesystem but by default we only fsync() to sync
# the OS cache lazily. The following configurations control the flush of data to disk.
# There are a few important trade-offs here:
#    1. Durability: Unflushed data may be lost if you are not using replication.
#    2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush.
#    3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to excessive seeks.
# The settings below allow one to configure the flush policy to flush data after a period of time or
# every N messages (or both). This can be done globally and overridden on a per-topic basis.

# The number of messages to accept before forcing a flush of data to disk
#log.flush.interval.messages=10000

# The maximum amount of time a message can sit in a log before we force a flush
#log.flush.interval.ms=1000

############################# Log Retention Policy #############################

# The following configurations control the disposal of log segments. The policy can
# be set to delete segments after a period of time, or after a given size has accumulated.
# A segment will be deleted whenever *either* of these criteria are met. Deletion always happens
# from the end of the log.

# The minimum age of a log file to be eligible for deletion due to age
##### 9 #####
log.retention.hours=168

# A size-based retention policy for logs. Segments are pruned from the log unless the remaining
# segments drop below log.retention.bytes. Functions independently of log.retention.hours.
#log.retention.bytes=1073741824

# The maximum size of a log segment file. When this size is reached a new log segment will be created.
##### 10 #####
log.segment.bytes=1073741824

# The interval at which log segments are checked to see if they can be deleted according
# to the retention policies
##### 11 #####
log.retention.check.interval.ms=300000

############################# Zookeeper #############################

# Zookeeper connection string (see zookeeper docs for details).
# This is a comma separated host:port pairs, each corresponding to a zk
# server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002".
# You can also append an optional chroot string to the urls to specify the
# root directory for all kafka znodes.
##### 12 #####
zookeeper.connect=localhost:2181

# Timeout in ms for connecting to zookeeper
##### 13 #####
zookeeper.connection.timeout.ms=18000


############################# Group Coordinator Settings #############################

# The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance.
# The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms.
# The default value for this is 3 seconds.
# We override this to 0 here as it makes for a better out-of-the-box experience for development and testing.
# However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup.
group.initial.rebalance.delay.ms=0
```

. 카프카 브로커 번호. 클러스터를 구축할 때 브로커들을 구분하기 위해 단 하나뿐인 번호로 설정
. 카프카 브로커와 통신을 위해 열어둘 인터페이스 IP, port, 프로토콜을 설정할 수 있다. 미설정시 모든 IP와 port에서 접속 가능
. 카프카 클러이언트 또는 카프카 커맨드 라인 툴에서 접속할 때 사용하는 브로커의 IP와 port 정보
. SASL_SSL, SASL_PLAIN 보안 설정 시 프로토콜 매핑
. 네트워크를 통한 처리를 할 때 사용할 네트워크 스레드 개수
. 카프카 브로커 내부에서 사용할 스레드 개수
. 통신을 통해 가져온 데이터를 파일로 저장할 디렉토리 위치 +
디렉토리가 생성되어 있지 않으면 오류가 발생할 수 있으므로 브로커 실행 전에 디렉토리 생성 여부를 확인
. 파티션 개수를 명시하지 않고 토픽을 생성할 때 기본 설정되는 파티션 개수 +
파티션 개수가 많아지면 병렬처리 데이터양 증가
. 카프카 브로커가 저장한 파일이 삭제되기까지 걸리는 시간 +
가장 작은 단위를 기준으로 하므로 log.retention.hours 보다는 log.retention.ms 값을 설정하여 운영하는 것을 추천 +
log.retention.ms 값을 -1로 설정하면 삭제하지 않음
. 카프카 브로커가 저장할 파일의 최대 크기를 지정,
지정한 크기보다 크면 새로운 파일이 생성
. 카프카 브로커가 저장할 파일을 삭제하기 위해 체크하는 간격
. 카프카 브로커와 연동할 주키퍼의 IP와 port
. 주키퍼의 세션 타임아웃 시간

*주키퍼(Zookeeper) 실행*

분산 코디네이션 서비스를 제공 +
카프카의 클러스터 설정 리더 정보, 컨트롤러 정보를 담고 있어 카프카를 실행하는 데 필요한 필수 애플리케이션

사용환경에서는 3대 이상의 서버로 구성하여 사용 +
실습에서는 1대만 실행, 1대만 실행하는 주키퍼를 'Quick-and-dirty single-node'라 부름

config/zookeeper.properties
```
...
# the directory where the snapshot is stored.
dataDir=/tmp/zookeeper
# the port at which the clients will connect
clientPort=2181
# disable the per-ip limit on the number of connections since this is a non-production config
maxClientCnxns=0
# Disable the adminserver by default to avoid port conflicts.
# Set the port to something non-conflicting if choosing to enable this
admin.enableServer=false
# admin.serverPort=8080
```

주키퍼 실행
```
bin/zookeeper-server-start.sh -daemon config/zookeeper.properties
```

jps 로 실행상태 확인

- jps : JVM 프로세스 상태를 보는 도구
- -m : main 메서드에 전달된 인자를 확인
- -v : JVM에 전달된 인자(힙 메모리 설정, log4j 설정 등)를 확인

```
$ jps -vm
...
21896 QuorumPeerMain config/zookeeper.properties -Xmx400m -XX:+UseG1GC -XX:MaxGCPauseMillis=20 -XX:InitiatingHeapOccupancyPercent=35 -XX:+ExplicitGCInvokesConcurrent -XX:MaxInlineLevel=15 -Djava.awt.headless=true -Xlog:gc*:file=/mnt/d/dev/kafka_2.13-3.1.0/bin/../logs/zookeeper-gc.log:time,tags:filecount=10,filesize=100M -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dkafka.logs.dir=/mnt/d/dev/kafka_2.13-3.1.0/bin/../logs -Dlog4j.configuration=file:bin/../config/log4j.properties
```

*카프카 브로커 실행*

```
bin/kafka-server-start.sh -daemon config/server.properties
```

실행 확인
```
$ jps -m
22295 Kafka config/server.properties
21896 QuorumPeerMain config/zookeeper.properties
22412 Jps -m
```

로그 확인
```
$ tail -f logs/server.log
[2022-03-20 04:38:04,203] INFO [/config/changes-event-process-thread]: Starting (kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread)
[2022-03-20 04:38:04,224] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Starting socket server acceptors and processors (kafka.network.SocketServer)
[2022-03-20 04:38:04,232] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
[2022-03-20 04:38:04,234] INFO [SocketServer listenerType=ZK_BROKER, nodeId=0] Started socket server acceptors and processors (kafka.network.SocketServer)
[2022-03-20 04:38:04,247] INFO Kafka version: 3.1.0 (org.apache.kafka.common.utils.AppInfoParser)
[2022-03-20 04:38:04,247] INFO Kafka commitId: 37edeed0777bacb3 (org.apache.kafka.common.utils.AppInfoParser)
[2022-03-20 04:38:04,248] INFO Kafka startTimeMs: 1647718684235 (org.apache.kafka.common.utils.AppInfoParser)
[2022-03-20 04:38:04,253] INFO [KafkaServer id=0] started (kafka.server.KafkaServer)
[2022-03-20 04:38:04,462] INFO [BrokerToControllerChannelManager broker=0 name=alterIsr]: Recorded new controller, from now on will use broker 127.0.0.1:9092 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread)
[2022-03-20 04:38:04,481] INFO [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use broker 127.0.0.1:9092 (id: 0 rack: null) (kafka.server.BrokerToControllerRequestThread)
```

#### 로컬 컴퓨터에서 카프카와 통신 확인

커맨드 라인 명령툴

```
$ ls bin
connect-distributed.sh        kafka-dump-log.sh              kafka-storage.sh
connect-mirror-maker.sh       kafka-features.sh              kafka-streams-application-reset.sh
connect-standalone.sh         kafka-get-offsets.sh           kafka-topics.sh
kafka-acls.sh                 kafka-leader-election.sh       kafka-transactions.sh
kafka-broker-api-versions.sh  kafka-log-dirs.sh              kafka-verifiable-consumer.sh
kafka-cluster.sh              kafka-metadata-shell.sh        kafka-verifiable-producer.sh
kafka-configs.sh              kafka-mirror-maker.sh          trogdor.sh
kafka-console-consumer.sh     kafka-producer-perf-test.sh    windows
kafka-console-producer.sh     kafka-reassign-partitions.sh   zookeeper-security-migration.sh
kafka-consumer-groups.sh      kafka-replica-verification.sh  zookeeper-server-start.sh
kafka-consumer-perf-test.sh   kafka-run-class.sh             zookeeper-server-stop.sh
kafka-delegation-tokens.sh    kafka-server-start.sh          zookeeper-shell.sh
kafka-delete-records.sh       kafka-server-stop.sh
```

kafka-broker-api-versions.sh 로 카프카 브로커 정보 조회
```
$ bin/kafka-broker-api-versions.sh --bootstrap-server 127.0.0.1:9092
127.0.0.1:9092 (id: 0 rack: null) -> (
        Produce(0): 0 to 9 [usable: 9],
        Fetch(1): 0 to 13 [usable: 13],
        ListOffsets(2): 0 to 7 [usable: 7],
        Metadata(3): 0 to 12 [usable: 12],
        LeaderAndIsr(4): 0 to 5 [usable: 5],
        StopReplica(5): 0 to 3 [usable: 3],
        UpdateMetadata(6): 0 to 7 [usable: 7],
        ControlledShutdown(7): 0 to 3 [usable: 3],
        OffsetCommit(8): 0 to 8 [usable: 8],
        OffsetFetch(9): 0 to 8 [usable: 8],
        FindCoordinator(10): 0 to 4 [usable: 4],
        JoinGroup(11): 0 to 7 [usable: 7],
        Heartbeat(12): 0 to 4 [usable: 4],
        LeaveGroup(13): 0 to 4 [usable: 4],
        SyncGroup(14): 0 to 5 [usable: 5],
        DescribeGroups(15): 0 to 5 [usable: 5],
        ListGroups(16): 0 to 4 [usable: 4],
        SaslHandshake(17): 0 to 1 [usable: 1],
        ApiVersions(18): 0 to 3 [usable: 3],
        CreateTopics(19): 0 to 7 [usable: 7],
        DeleteTopics(20): 0 to 6 [usable: 6],
        DeleteRecords(21): 0 to 2 [usable: 2],
        InitProducerId(22): 0 to 4 [usable: 4],
        OffsetForLeaderEpoch(23): 0 to 4 [usable: 4],
        AddPartitionsToTxn(24): 0 to 3 [usable: 3],
        AddOffsetsToTxn(25): 0 to 3 [usable: 3],
        EndTxn(26): 0 to 3 [usable: 3],
        WriteTxnMarkers(27): 0 to 1 [usable: 1],
        TxnOffsetCommit(28): 0 to 3 [usable: 3],
        DescribeAcls(29): 0 to 2 [usable: 2],
        CreateAcls(30): 0 to 2 [usable: 2],
        DeleteAcls(31): 0 to 2 [usable: 2],
        DescribeConfigs(32): 0 to 4 [usable: 4],
        AlterConfigs(33): 0 to 2 [usable: 2],
        AlterReplicaLogDirs(34): 0 to 2 [usable: 2],
        DescribeLogDirs(35): 0 to 2 [usable: 2],
        SaslAuthenticate(36): 0 to 2 [usable: 2],
        CreatePartitions(37): 0 to 3 [usable: 3],
        CreateDelegationToken(38): 0 to 2 [usable: 2],
        RenewDelegationToken(39): 0 to 2 [usable: 2],
        ExpireDelegationToken(40): 0 to 2 [usable: 2],
        DescribeDelegationToken(41): 0 to 2 [usable: 2],
        DeleteGroups(42): 0 to 2 [usable: 2],
        ElectLeaders(43): 0 to 2 [usable: 2],
        IncrementalAlterConfigs(44): 0 to 1 [usable: 1],
        AlterPartitionReassignments(45): 0 [usable: 0],
        ListPartitionReassignments(46): 0 [usable: 0],
        OffsetDelete(47): 0 [usable: 0],
        DescribeClientQuotas(48): 0 to 1 [usable: 1],
        AlterClientQuotas(49): 0 to 1 [usable: 1],
        DescribeUserScramCredentials(50): 0 [usable: 0],
        AlterUserScramCredentials(51): 0 [usable: 0],
        AlterIsr(56): 0 [usable: 0],
        UpdateFeatures(57): 0 [usable: 0],
        DescribeCluster(60): 0 [usable: 0],
        DescribeProducers(61): 0 [usable: 0],
        DescribeTransactions(65): 0 [usable: 0],
        ListTransactions(66): 0 [usable: 0],
        AllocateProducerIds(67): 0 [usable: 0]
)
```

CAUTION: 카프카 브로커와 로컬 커맨드 라인 툴 버전을 맞춘다.


*테스트 편의를 위한 hosts 설정*

```
$ sudo vi /etc/hosts
127.0.0.1   my-kafka
```

### 2.2 카프카 커맨드 라인 툴

- 토픽 생성
- 토픽 수정
- 데이터 전송(프로듀서)
- 데이터 수신(컨슈머)

#### kafka-topics.sh

토피(topic) 관련 명령 실행

*토픽* +
카프카에서 데이터를 구분하는 가장 기본적인 개념. RDBMS의 테이블과 유사 +
토픽에는 파티션(partition)이 존재하고, 최소 1개. +
파티션을 통해 한 번에 처리할 수 있는 데이터의 양을 늘릴 수 있고 토픽 내부에서도 파티션을 통해 데이터의 종류를 나누어 처리할 수 있다.

TIP: *토픽을 생성하는 2가지 방법* +
1. 카프카 컴슈머 또는 프로듀서가 카프카 브로커에 생성되지 않은 토픽에 대해 데이터를 요청할 때 +
2. 커맨드 라인 툴로 명시적으로 토픽을 생성하는 것 +
토픽을 효과적으로 유지보수하기 위해서는 토픽을 명시적으로 생성하는 것을 추천. 토픽마다 처리되어야 하는 데이터의 특성이 다르기 때문. +
+
토픽을 생성할 때는 데이터의 특성에 따라 옵션을 다르게 설정할 수 있다.
예를 들어, 동시 데이터 처리량이 많아야 하는 토픽의 경우 파티션의 개수를 100으로 설정할 수 있다.
단기간 데이터 처리만 필요한 경우에는 토픽에 들어온 데이터의 보관기간 옵션을 짧게 설정할 수도 있다.
그러므로 토픽에 들어오는 데이터양과 병렬로 처리되어야 하는 용량을 잘 파악하여 생성하는 것이 중요하다.

*토픽 생성*

hello.kafka 토픽 생성
```
$ bin/kafka-topics.sh \
  --create \
  --bootstrap-server my-kafka:9092 \
  --topic hello.kafka

WARNING: Due to limitations in metric names, topics with a period ('.') or underscore ('_') could collide. To avoid issues it is best to use either, but not both.
Created topic hello.kafka.
```

. --create : 토픽 생성 명시
. --bootstrap-server : 토픽을 생성할 카프카 클러스터를 구성하는 브로커들의 IP와 port
. --topic : 토픽 이름, 내부 데이터가 무엇이 있는지 유추가 가능할 정도로 자세히 적는 것을 추천

필수 값: 카프카 클러스터 정보, 토픽 이름

나머지 옵션들(파티션 개수, 복제 개수 등)은 브로커에 설정된 기본값으로 생성

hello.kafka2 토픽 생성 : 파티션 개수, 토픽의 파티션 복제 개수 및 토픽 데이터 유지 기간 옵션 지정하여 토픽 생성
```
$ bin/kafka-topics.sh \
  --create \
  --bootstrap-server my-kafka:9092 \
  --partitions 3 \
  --replication-factor 1 \
  --config retention.ms=172800000 \
  --topic hello.kafka2
```

. --partition : 파티션 개수, 최소 개수는 1개, 미지정시 카프카 브로터 설정파일(config/server.properties)의 num.partitions 옵션값 사용
. --replication-factor : 토픽의 파티션을 복제할 복제 개수. 1은 복제를 하지 않고 사용한다는 의미다.
2이면 1개의 복제본을 사용하겠다는 의미이다. 파티션의 데이터는 각 브로커마다 저장된다.
한 개의 브로커에 장애가 발생하더라도 나머지 한 개 브로커에 저장된 데이터를 사용하여 안전하게 데이터 처리를 지속적으로 할 수 있다.
복제 개수의 최소 설정은 1이고 최대 설정은 통신하는 카프카 클러스터의 브로커 개수이다.
실제 업무환경에서는 3개 이상의 카프카 브로커로 운영하는 것이 일반적으로 2 또는 3으로 복제 개수를 설정하여 사용한다.
미 지정시 카프카 브로커 설정에 있는 default.replication.factor 옵션값(default: 1)을 사용 +
. --config : kafka-topics.sh 명령에 포한되지 않은 추가적인 설정을 할 수 있다. +
retention.ms는 토픽의 데이터를 유지하는 기간을 뜻한다. 172800000ms는 2일을 ms(밀리세컨드) 단위로 나타낸 것이다.
2일이 지난 토픽의 데이터는 삭제된다.

*토픽 리스트 조회*

```
$ bin/kafka-topics.sh --bootstrap-server my-kafka:9092 --list
hello.kafka
hello.kafka2
```

*토픽 상세 조회*

```
$ bin/kafka-topics.sh --bootstrap-server my-kafka:9092 --describe --topic hello.kafka2
Topic: hello.kafka2     TopicId: eML6CYoJSwacw2znqWcayw PartitionCount: 3       ReplicationFactor: 1    Configs: segment.bytes=1073741824,retention.ms=172800000
        Topic: hello.kafka2     Partition: 0    Leader: 0       Replicas: 0     Isr: 0
        Topic: hello.kafka2     Partition: 1    Leader: 0       Replicas: 0     Isr: 0
        Topic: hello.kafka2     Partition: 2    Leader: 0       Replicas: 0     Isr: 0
```

Leader : 파티션이 위치한 브로커

리더 파티션이 일부 브로커에 몰려있는 경우 카프카 클러스터 부하가 특정 브로커들로 몰릴 수 있다.
부하가 분산되지 못하면 데이터 통신 쏠림 현상으로 인해 네트워크 대역의 이슈가 생길 수 있다.

*토픽 옵션 수정*

kafka-topics.sh 또는 kafka-configs.sh 사용

파티션 개수 변경을 하려면 kafka-topics.sh를 사용해야 하고 토픽 삭제 정책인 리텐션 기간을 변경하려면 kafka-configs.sh를 사용해야 한다.
토픽 설정 옵션이 파편화된 이유는 토픽에 대한 정보를 관리하는 일부 로직이 다른 명령어로 넘어갔기 때문이다.

토픽 옵션 중 다이나믹 토픽 옵션(dynamic topic config)이라고 정의되는 일부 옵션들(log.segment.bytes, log.retention.ms 등)은 kafka-configs.sh를 통해 수정할 수 있다.

hello.kafka 토픽의 파티션을 4개로 늘리고, 리텐션 기간을 86400000ms(1일)로 변경
```
# 1
$ bin/kafka-topics.sh --bootstrap-server my-kafka:9092 --describe --topic hello.kafka
Topic: hello.kafka      TopicId: 0UDpRVKOSAW9oozv3Wup1Q PartitionCount: 1       ReplicationFactor: 1    Configs: segment.bytes=1073741824
        Topic: hello.kafka      Partition: 0    Leader: 0       Replicas: 0     Isr: 0

$ bin/kafka-topics.sh --bootstrap-server my-kafka:9092 \
  --topic hello.kafka \
  --alter \
  --partitions 4

# 2
$ bin/kafka-topics.sh --bootstrap-server my-kafka:9092 --describe --topic hello.kafka
Topic: hello.kafka      TopicId: 0UDpRVKOSAW9oozv3Wup1Q PartitionCount: 4       ReplicationFactor: 1    Configs: segment.bytes=1073741824
        Topic: hello.kafka      Partition: 0    Leader: 0       Replicas: 0     Isr: 0
        Topic: hello.kafka      Partition: 1    Leader: 0       Replicas: 0     Isr: 0
        Topic: hello.kafka      Partition: 2    Leader: 0       Replicas: 0     Isr: 0
        Topic: hello.kafka      Partition: 3    Leader: 0       Replicas: 0     Isr: 0

# 3
$ bin/kafka-configs.sh --bootstrap-server my-kafka:9092 \
  --entity-type topics \
  --entity-name hello.kafka \
  --alter --add-config retention.ms=86400000
Completed updating config for topic hello.kafka.

# 4
$ bin/kafka-configs.sh --bootstrap-server my-kafka:9092 \
  --entity-type topics \
  --entity-name hello.kafka \
  --describe
Dynamic configs for topic hello.kafka are:
  retention.ms=86400000 sensitive=false synonyms={DYNAMIC_TOPIC_CONFIG:retention.ms=86400000}
```

. --alter, --partitions 옵션을 사용하여 파티션 개수 변경
토픽의 파티션을 늘릴수 있지만 줄일 수는 없다.
. 파티션이 4개로 변경. 파티션 번호는 0부터 시작하고 1씩 증가
. retention.ms 수정을 위해 kafka-configs.sh와 --alter, --add-config 옵션 사용 +
--add-config 옵션은 존재하는 설정값은 변경하고 존재하지 않는 설정값은 신규로 추가
. 다이나믹 토픽 옵션 retention.ms가 변경된 것은 kafka-configs.sh와 --describe 옵션을 통해 확인

#### 2.2.2 kafka-console-producer.sh

kafka-console-producer.sh 를 이용해서 토픽에 데이터를 넣기

토픽에 넣는 데이터는 '레코드(record)'라고 부르며 메시지 키(key)와 메시지 값(value)으로 구성

*메시지 값만 가지는 레코드 전송*
메시지 값만 보내면 메시지 키는 자바의 null로 기본 설정되어 브로커로 전송
```
$ bin/kafka-console-producer.sh --bootstrap-server my-kafka:9092 --topic hello.kafka
>hello
>kafka
>0
>1
>2
>3
>4
>5
```

kafka-console-producer.sh로 전송되는 레코드 값은 UTF-8을 기반으로 Byte로 변환되고 ByteArraySerializer로만 직렬화된다 +
즉, String이 아닌 타입으로는 직렬화하여 전송할 수 없다. +
다른 타입으로 직렬화하여 데이터를 브로커로 전송하고 싶다면 카프카 프로듀서 애플리케이션을 직접 개발해야 한다.

*메시지 키를 가지는 레코드 전송*
```
$ bin/kafka-console-producer.sh --bootstrap-server my-kafka:9092 --topic hello.kafka \
  --property "parse.key=true" \
  --property "key.separator=:"
>key1:no1
>key2:no2
>key3:no3
```

- --property "parse.key=true" : parse.key를 true로 두면 레코드를 전송할 때 메시지 키를 추가 가능
- --property "key.separator=:" : 메시지 키와 메시지 값을 구분하는 구분자를 선언, 기본 설정은 Tab delimiter(\t), 구분자 없이 전송하면 KafkaException과 함께 종료

레코드는 토픽의 파티션에 저장된다. +
메시지 키가 null인 경우에는 프로듀서가 파티션으로 전송할 때 레코드 배치 단위(레코드 전송 묶음)로 라운드 로빈으로 전송 +
메시지 키가 존재하는 경우에는 키의 해시값을 작성하여 존재하는 파티션 중 한개에 할당. 이에 메시지 키가 동일한 경우 동일한 파티션으로 전송

다만, 이런 메시지 키와 파티션 할당은 프로듀서에서 설정된 파티셔너에 의해 결정되는데, 기본 파티셔너의 경우 이와 같은 동작을 보장한다.
커스텀 파티셔너를 사용할 경우에는 메시지 키에 따른 파티션 할당이 다르게 동작

TIP: *파티션 개수가 늘어나면 새로 프로듀싱되는 레코드들은 어느 파티션으로 갈까?* +
메시지 키를 가진 레코드의 경우 파티션이 추가되면 파티션과 메시지 키의 일관성이 보장되지 않는다.
즉, 이전에 미시지 키가 파티션 0번에 들어갔다면 파티션을 늘린 뒤에는 파티션 0번으로 간다는 보장이 없다.
만약 파티션을 추가하더라도 이전에 사용하던 메시지 키와 일관성을 보장하고 싶다면 파티셔너를 만들어서 운영해야 한다.
기본 파티셔너와 커스텀 파티셔너에 대한 자세한 설명은 챕터 3에서 확인할 수 있다.

#### 2.2.3 kafka-console.consumer.sh

kafka-console.consumer.sh 로 hello.kafka 토픽으로 전송된 데이터를 확인

```
$ bin/kafka-console-consumer.sh --bootstrap-server my-kafka:9092 \
  --topic hello.kafka \
  --from-beginning
kafka
5
no2
3
4
no3
hello
0
1
2
no1
```

- --from-beginning : 토픽에 저장된 가장 처음 데이터부터 출력

데이터의 메시지 키와 메시지 값을 확인, --property 옵션 사용
```
$ bin/kafka-console-consumer.sh --bootstrap-server my-kafka:9092 \
  --topic hello.kafka \
  --property print.key=true \
  --property key.separator="-" \
  --group hello-group \
  --from-beginning
null-kafka
null-5
key2-no2
null-3
null-4
key3-no3
null-hello
null-0
null-1
null-2
key1-no1
```

- --property print.key=true : 메시지 키 확인 (기본값: false)
- --property key.separator="-" : 메시키 키 값 구분 (기본값: tab(\t))
- --group hello-group : --group 옵션을 통해 신규 컨슈머 그룹(consumer group)을 생성 +
컨슈머 그룹은 1개 이상의 컨슈머로 이루어져 있다. 이 컨슈머 그룹을 통해 가져간 토픽의 메시지는 가져간 메시지에 대한 커밋(commit)을 한다.
커밋이란 컨슈머가 특정 레코드까지 처리를 완료했다고 레코드의 오프셋 번호를 카프카 브로커에 저장하는 것이다.
커밋 정보는 __consumer_offsets 이름의 내부 토픽에 저장된다.

kafka-console-producer.sh로 전송했던 데이터의 순서가 출력되는 순서와 다르다.
이는 카프카의 핵심인 파티션 개념 떄문에 생기는 현상이다.
kafka-console-consumer.sh 명령어를 통해 토픽의 데이터를 가져가게 되면 토픽의 모든 파티션으로부터 동일한 중요도로 데이터를 가져간다.
이로 인해 프로듀서가 토픽에 넣은 데이터의 순서와 컨슈머가 토픽에서 가져간 데이터의 순서가 달라지게 되는 것이다.

만약 토픽에 넣은 데이터의 순서를 보장하고 싶다면 가장 좋은 방법은 파티션 1개로 구성된 토픽을 만드는 것이다.
한 개의 파티션에서는 데이터의 순서를 보장하기 때문이다.

#### 2.2.4 kafka-consumer-groups.sh

컨슈머 그룹은 따로 생성하는 명령을 날리지 않고 컨슈머를 동작할 때 컨슈머 그룹 이름을 지정하면 새로 생성된다.
생성된 컨슈머 그룹의 리스트는 kafka-consumer-groups.sh 명령어로 확인할 수 있다.

*컨슈머 그룹 목록 조회*
```
$ bin/kafka-consumer-groups.sh --bootstrap-server my-kafka:9092 --list
hello-group
```

- --list: 컨슈머 그룹의 리스트 확인

*컨슈머 그룹 확인*
```
$ bin/kafka-consumer-groups.sh --bootstrap-server my-kafka:9092 \
  --group hello-group \
  --describe

Consumer group 'hello-group' has no active members.

GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID
hello-group     hello.kafka     0          5               5               0               -               -               -
hello-group     hello.kafka     1          1               1               0               -               -               -
hello-group     hello.kafka     2          2               2               0               -               -               -
hello-group     hello.kafka     3          3               3               0               -               -               -
```

- --group hello-group : 컨슈머 그룹 지정
- --describe : 컨슈머 그룹 상세 조회
- GROUP, TOPIC, PARTITION : 마지막으로 커밋한 토픽과 파티션을 나타낸다.
- CURRENT-OFFSET : 컨슈머 그룹이 가져간 토픽의 파티션에 가장 최신 오프셋(offset)이 몇 번인지 나타낸다.
오프셋이란 파티션의 각 레코드에 할당된 번호다. 이 번호는 데이터가 파티션에 들어올 때마다 1씩 증가한다.
- LOG-END-OFFSET : 컨슈머 그룹의 컨슈머가 어느 오프셋까지 커밋했는지 알 수 있다.
- LAG : 랙은 컨슈머 그룹이 토픽의 파티션에 있는 데이터를 가져가는 데에 얼마나 지연이 발생하는지 나타내는 지표이다.
랙은 컨슈머 그룹이 커밋한 오프셋과 해당 파티션의 가장 최신 오프셋 간의 차이다.
- CONSUMER-ID : 컨슈머의 토픽(파티션) 할당을 카프카 내부적으로 구분하기 위해 사용하는 id이다.
이 값은 client id에 uuid(universally unique identifier) 값을 붙여서 자동 할당되어 유니크한 값으로 설정된다.
- HOST : 컨슈머가 동작하는 host명을 출력한다. 이 값을 통해 카프카에 붙은 컨슈머의 호스트명 또는 IP를 알 수 있다.
- CLIENT-ID : 컨슈머에 할당된 id이다. 이 값ㄷ은 사용자가 지정할 수 있으며 지정하지 않으면 자동 생성된다.

#### 2.2.5 kafka-verifiable-producer, consumer.sh
kafka-verifiable로 시작하는 2개의 스크립트를 사용하면 String 타입 메시지 값을 코드 없이 주고받을 수 있다.
카프카 클러스터 설치가 완료된 이후에 토픽에 데이터를 전송하여 간단한 네트워크 통신 테스트를 할 때 유용하다.

*데이터 전송, kafka-verifiable-producer.sh*
```
$ bin/kafka-verifiable-producer.sh --bootstrap-server my-kafka:9092 \
  --max-messages 10 \           # 1
  --topic verify-test           # 2
# 3
{"timestamp":1647725976808,"name":"startup_complete"}
[2022-03-20 06:39:37,160] WARN [Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {verify-test=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)
# 4
{"timestamp":1647725977309,"name":"producer_send_success","key":null,"value":"0","offset":0,"topic":"verify-test","partition":0}
{"timestamp":1647725977311,"name":"producer_send_success","key":null,"value":"1","offset":1,"topic":"verify-test","partition":0}
{"timestamp":1647725977312,"name":"producer_send_success","key":null,"value":"2","offset":2,"topic":"verify-test","partition":0}
{"timestamp":1647725977312,"name":"producer_send_success","key":null,"value":"3","offset":3,"topic":"verify-test","partition":0}
{"timestamp":1647725977313,"name":"producer_send_success","key":null,"value":"4","offset":4,"topic":"verify-test","partition":0}
{"timestamp":1647725977313,"name":"producer_send_success","key":null,"value":"5","offset":5,"topic":"verify-test","partition":0}
{"timestamp":1647725977313,"name":"producer_send_success","key":null,"value":"6","offset":6,"topic":"verify-test","partition":0}
{"timestamp":1647725977313,"name":"producer_send_success","key":null,"value":"7","offset":7,"topic":"verify-test","partition":0}
{"timestamp":1647725977313,"name":"producer_send_success","key":null,"value":"8","offset":8,"topic":"verify-test","partition":0}
{"timestamp":1647725977313,"name":"producer_send_success","key":null,"value":"9","offset":9,"topic":"verify-test","partition":0}
{"timestamp":1647725977322,"name":"shutdown_complete"}
# 5
{"timestamp":1647725977323,"name":"tool_data","sent":10,"acked":10,"target_throughput":-1,"avg_throughput":19.37984496124031}
```

. --max-messages 10 : 데이터 개수, -1 옵션 지정시 kafka-verifiable-producer.sh가 종료될 때까지 계속 데이터를 토픽으로 보낸다.
. --topic verify-test : 데이터를 받을 토픽
. startup_complete : 최초 실행 시점
. producer_send_success : 메시지별 보낸 시간과 메시지 키, 메시지 값, 토픽, 저장된 파티션, 저장된 오프셋 번호 출력
. tool_data : 10개 데이터가 모두 전송된 이후 통계값

*데이터 확인, kafka-verifiable-consumer.sh*
```
$ bin/kafka-verifiable-consumer.sh --bootstrap-server my-kafka:9092 \
  --topic verify-test \        # 1
  --group-id test-group        # 2
{"timestamp":1647726563201,"name":"startup_complete"}
{"timestamp":1647726563869,"name":"partitions_assigned","partitions":[{"topic":"verify-test","partition":0}]}
{"timestamp":1647726563950,"name":"records_consumed","count":10,"partitions":[{"topic":"verify-test","partition":0,"count":10,"minOffset":0,"maxOffset":9}]}
{"timestamp":1647726563957,"name":"offsets_committed","offsets":[{"topic":"verify-test","partition":0,"offset":10}],"success":true}
```

. --topic verify-test : 토픽 지정
. --group-id test-group : 컨슈머 그룹 지정
. startup_complete : 컨슈머 시작
. partitions_assigned : 컨슈머는 토픽에서 데이터를 가져오기 위해 파티션에 할당하는 과정을 거친다. 여기서는 0번 파티션이 할당
. records_consumed, offsets_committed : 컨슈머는 한 번에 다수의 메시지를 가져와서 처리하므로 한 번에 10개의 메시지를 정상적으로 받았음을 알 수 있다.
메시지 수신 이후 10번 오프셋 커밋 여부도 확인할 수 있다.

#### 2.2.6 kafka-delete-records.sh

가장 오래된 데이터(가장 낮은 숫자의 오프셋)부터 특정 시점의 오프셋 까지 삭제할 수 있다.

*데이터 삭제*
0부터 10 오프셋 데이터까지 삭제
```
$ vi delete-topic.json                      # 1
{"partitions":[{"topic":"verify-test", "partition":0, "offset":10}],"version":1}
$ bin/kafka-delete-records.sh --bootstrap-server my-kafka:9092 \
  --offset-json-file delete-topic.json      # 2
# 3
Executing records delete operation
Records delete operation completed:
partition: verify-test-0        low_watermark: 10
```

. delete-topic.json : 삭제 데이터 정보를 파일로 저장. 삭제하려는 토픽, 파티션, 오프셋
. --offset-json-file : 삭제 토픽, 파티션, 오프셋 정보를 담은 파일 지정
. 삭제가 완료되면 각 파티션에서 삭제된 오프셋 정보를 출력

CAUTION: 토픽의 특정 레코드 하나만 삭제되는 것이 아니라 파티션에 존재하는 가장 오래된 오프셋부터 지정한 오프셋까지 삭제된다. +
카프카에서는 토픽의 파티션에 저장된 특정 데이터만 삭제할 수는 없다는 점을 명심해야 한다.

### 개인실습 - Local에 클러스터(Cluster) 구성

* link:https://epicdevs.com/20[참고1 - zookeeper cluster]
* link:https://twofootdog.tistory.com/89[참고2 - zookeeper cluster]
* link:https://namsick96.github.io/kafka/Kafka_Cluster/[참고3 - Kafka cluster]
* link:https://skysoo1111.tistory.com/75[참고4 - kafka test command]

#### Zookeeper 클러스터 구축

Zookeeper 버전 3.6.3

. /tmp/zk1, /tmp/zk2, /tmp/zk3 폴더 생성 +
+
```
mkdir -p /tmp/zk1
mkdir -p /tmp/zk2
mkdir -p /tmp/zk3
```
. 각 폴더에 'myid'파일을 생성하고 각 내용을 1,2,3으로 생성 +
Zookeeper 서버 번호를 myid 파일을 통해서 통보
+
./tmp/zk1/myid
```
1
```
+
./tmp/zk2/myid
```
2
```
+
./tmp/zk3/myid
```
3
```
. bin/zk1.properties, bin/zk2.properties, bin/zk3.properties 파일을 아래와 같이 생성+
+
.$KAFKA_HOME/bin/zk1.properties
```
tickTime=2000
initLimit=10
syncLimit=5
dataDir=/tmp/zk1
clientPort=2181
server.1=127.0.0.1:2888:3888
server.2=127.0.0.1:2889:3889
server.3=127.0.0.1:2890:3890
```
+
.$KAFKA_HOME/bin/zk2.properties
```
tickTime=2000
initLimit=10
syncLimit=5
dataDir=/tmp/zk2
clientPort=2182
server.1=127.0.0.1:2888:3888
server.2=127.0.0.1:2889:3889
server.3=127.0.0.1:2890:3890
```
+
.$KAFKA_HOME/bin/zk3.properties
```
tickTime=2000
initLimit=10
syncLimit=5
dataDir=/tmp/zk3
clientPort=2183
server.1=127.0.0.1:2888:3888
server.2=127.0.0.1:2889:3889
server.3=127.0.0.1:2890:3890
```
. '$KAFKA_HOME'에서 Zookeeper 실행 +
+
```
bin/zookeeper-server-start.sh -daemon config/zk1.properties
bin/zookeeper-server-start.sh -daemon config/zk2.properties
bin/zookeeper-server-start.sh -daemon config/zk3.properties
```

[NOTE]
====
**Configuration**

- tickTime : 주키퍼가 사용하는 시간에 대한 기본 측정 단위(밀리초)
- initLimit : 팔로워가 리더와 초기에 연결하는 시간에 대한 타임아웃 tick의 수
- syncLimit : 팔로워가 리더와 동기화 하는 시간에 대한 타임아웃 tick의 수(주키퍼에 저장된 데이터가 크면 더 크게 잡아야 함)
- dataDir : 주키퍼의 트랜잭션 로그와 스냅샷이 저장되는 저장경로.
- clientPort : 주키퍼 포트
- server.x : 주키퍼 앙상블 구성을 위한 서버 설정. server.myid 형식으로 사용. 기본 포트는 2888:3888.
첫 번째 port는 follower가 leader에 접속하기 위해서 사용. 두 번째 port는 리더 선출을 위해서 사용
====

#### Kafka 클러스터 구축

. 'KAFKA_HEAP_OPTS' 환경 변수에 HEAP 메모리 설정 및 .bashrc에 추가 +
+
```
export KAFKA_HEAP_OPTS="-Xmx400m -Xms400m"
echo 'export KAFKA_HEAP_OPTS="'$KAFKA_HEAP_OPTS'"' | tee -a $HOME/.bashrc
```
. config/server.properties 를 config/server1.properties, config/server2.properties, config/server3.properties 로 복사 후.
각 파일의 아래부분을 수정
+
.config/server1.properties
```
borker.id=1
listeners=PLAINTEXT://localhost:9092
advertised.listeners=PLAINTEXT://127.0.0.1:9092
log.dirs=/tmp/kafka-1-logs
zookeeper.connect=127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183/test-kafka
```
+
.config/server2.properties
```
borker.id=2
listeners=PLAINTEXT://localhost:9093
advertised.listeners=PLAINTEXT://127.0.0.1:9093
log.dirs=/tmp/kafka-2-logs
zookeeper.connect=127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183/test-kafka
```
+
.config/server3.properties
```
borker.id=3
listeners=PLAINTEXT://localhost:9094
advertised.listeners=PLAINTEXT://127.0.0.1:9094
log.dirs=/tmp/kafka-3-logs
zookeeper.connect=127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183/test-kafka
```
. Kafka 실행
+
```
./bin/kafka-server-start.sh -daemon ./config/server1.properties
./bin/kafka-server-start.sh -daemon ./config/server2.properties
./bin/kafka-server-start.sh -daemon ./config/server3.properties
```

#### 클러스터 테스트
Topic(test) 생성
```
$ bin/kafka-topics.sh --create --bootstrap-server localhost:9092,localhost:9093,localhost:9094 --replication-factor 3 --partitions 1 --topic test
Created topic test.
```

Topic 조회
```
$ bin/kafka-topics.sh --list --bootstrap-server localhost:9092,localhost:9093,localhost:9094
test
```

Topic 상세조회
```
$ bin/kafka-topics.sh --describe --topic test --bootstrap-server localhost:9092,localhost:9093,localhost:9094
Topic: test     TopicId: n3nYAgNIQ8aIcxrMbcn24A PartitionCount: 1       ReplicationFactor: 3    Configs: segment.bytes=1073741824
        Topic: test     Partition: 0    Leader: 2       Replicas: 2,3,1 Isr: 2,3,1
```

Partition 1 -> 4 수정
```
$ bin/kafka-topics.sh --bootstrap-server localhost:9092,localhost:9093,localhost:9094 \
  --topic test \
  --alter \
  --partitions 4
```

Topic 상세조회
```
$ bin/kafka-topics.sh --describe --topic test --bootstrap-server localhost:9092,localhost:9093,localhost:9094
Topic: test     TopicId: n3nYAgNIQ8aIcxrMbcn24A PartitionCount: 4       ReplicationFactor: 3    Configs: segment.bytes=1073741824
        Topic: test     Partition: 0    Leader: 2       Replicas: 2,3,1 Isr: 2,3,1
        Topic: test     Partition: 1    Leader: 3       Replicas: 3,2,1 Isr: 3,2,1
        Topic: test     Partition: 2    Leader: 1       Replicas: 1,3,2 Isr: 1,3,2
        Topic: test     Partition: 3    Leader: 2       Replicas: 2,3,1 Isr: 2,3,1
```

Produce Messages
```
$ bin/kafka-verifiable-producer.sh --bootstrap-server localhost:9092,localhost:9093,localhost:9094 \
  --max-messages 10 \
  --topic test
{"timestamp":1647887422673,"name":"startup_complete"}
{"timestamp":1647887423130,"name":"producer_send_success","key":null,"value":"0","offset":0,"topic":"test","partition":1}
{"timestamp":1647887423133,"name":"producer_send_success","key":null,"value":"1","offset":1,"topic":"test","partition":1}
{"timestamp":1647887423133,"name":"producer_send_success","key":null,"value":"2","offset":2,"topic":"test","partition":1}
{"timestamp":1647887423133,"name":"producer_send_success","key":null,"value":"3","offset":3,"topic":"test","partition":1}
{"timestamp":1647887423133,"name":"producer_send_success","key":null,"value":"4","offset":4,"topic":"test","partition":1}
{"timestamp":1647887423134,"name":"producer_send_success","key":null,"value":"5","offset":5,"topic":"test","partition":1}
{"timestamp":1647887423134,"name":"producer_send_success","key":null,"value":"6","offset":6,"topic":"test","partition":1}
{"timestamp":1647887423134,"name":"producer_send_success","key":null,"value":"7","offset":7,"topic":"test","partition":1}
{"timestamp":1647887423134,"name":"producer_send_success","key":null,"value":"8","offset":8,"topic":"test","partition":1}
{"timestamp":1647887423134,"name":"producer_send_success","key":null,"value":"9","offset":9,"topic":"test","partition":1}
{"timestamp":1647887423143,"name":"shutdown_complete"}
{"timestamp":1647887423144,"name":"tool_data","sent":10,"acked":10,"target_throughput":-1,"avg_throughput":21.09704641350211}
```

Consume Messages
```
$ bin/kafka-verifiable-consumer.sh --bootstrap-server localhost:9092,localhost:9093,localhost:9094 \
  --topic test \
  --group-id test-group
{"timestamp":1647887503349,"name":"startup_complete"}
{"timestamp":1647887504576,"name":"partitions_assigned","partitions":[{"topic":"test","partition":1},{"topic":"test","partition":0},{"topic":"test","partition":3},{"topic":"test","partition":2}]}
{"timestamp":1647887504683,"name":"records_consumed","count":10,"partitions":[{"topic":"test","partition":1,"count":10,"minOffset":0,"maxOffset":9}]}
{"timestamp":1647887504702,"name":"offsets_committed","offsets":[{"topic":"test","partition":1,"offset":10}],"success":true}
```

```
$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092,localhost:9093,localhost:9094 \
opic tes>   --topic test \
>   --property print.key=true \
>   --property key.separator=":" \
>   --group test-group2 \
>   --from-beginning
null:0
null:1
null:2
null:3
null:4
null:5
null:6
null:7
null:8
null:9
```



##### Clustered(Multi-server) Setup
link:https://zookeeper.apache.org/doc/r3.6.3/zookeeperAdmin.html[ZooKeeper Administrator's Guide 3.5.8] +
link:https://zookeeper.apache.org/doc/r3.6.3/zookeeperAdmin.html#sc_zkMulitServerSetup[Clustered(Multi-server) Setup]

For reliable ZooKeeper service, you should deploy ZooKeeper in a cluster known as an ensemble. As long as a majority of the ensemble are up, the service will be available. Because Zookeeper requires a majority, it is best to use an odd number of machines. For example, with four machines ZooKeeper can only handle the failure of a single machine; if two machines fail, the remaining two machines do not constitute a majority. However, with five machines ZooKeeper can handle the failure of two machines.

[NOTE]
====
As mentioned in the ZooKeeper Getting Started Guide , a minimum of three servers are required for a fault tolerant clustered setup, and it is strongly recommended that you have an odd number of servers.

Usually three servers is more than enough for a production install, but for maximum reliability during maintenance, you may wish to install five servers. With three servers, if you perform maintenance on one of them, you are vulnerable to a failure on one of the other two servers during that maintenance. If you have five of them running, you can take one down for maintenance, and know that you're still OK if one of the other four suddenly fails.

Your redundancy considerations should include all aspects of your environment. If you have three ZooKeeper servers, but their network cables are all plugged into the same network switch, then the failure of that switch will take down your entire ensemble.
====

Here are the steps to setting a server that will be part of an ensemble. These steps should be performed on every host in the ensemble:

. Install the Java JDK. You can use the native packaging system for your system, or download the JDK from: http://java.sun.com/javase/downloads/index.jsp
. Set the Java heap size. This is very important to avoid swapping, which will seriously degrade ZooKeeper performance. To determine the correct value, use load tests, and make sure you are well below the usage limit that would cause you to swap. Be conservative - use a maximum heap size of 3GB for a 4GB machine.
. Install the ZooKeeper Server Package. It can be downloaded from: http://zookeeper.apache.org/releases.html
. Create a configuration file. This file can be called anything. Use the following settings as a starting point: +
+
```
tickTime=2000
dataDir=/var/lib/zookeeper/
clientPort=2181
initLimit=5
syncLimit=2
server.1=zoo1:2888:3888
server.2=zoo2:2888:3888
server.3=zoo3:2888:3888
```
+
You can find the meanings of these and other configuration settings in the section link:https://zookeeper.apache.org/doc/r3.5.8/zookeeperAdmin.html#sc_configuration[Configuration Parameters].
A word though about a few here: Every machine that is part of the ZooKeeper ensemble should know about every other machine in the ensemble.
You accomplish this with the series of lines of the form **server.id=host:port:port**.
The parameters **host** and **port** are straightforward.
You attribute the server id to each machine by creating a file named myid, one for each server, which resides in that server's data directory, as specified by the configuration file parameter **dataDir**.
. The myid file consists of a single line containing only the text of that machine's id.
So myid of server 1 would contain the text "1" and nothing else.
The id must be unique within the ensemble and should have a value between 1 and 255.
**IMPORTANT**: if you enable extended features such as TTL Nodes (see below) the id must be between 1 and 254 due to internal limitations.
. If your configuration file is set up, you can start a ZooKeeper server: +
+
```
$ java -cp zookeeper.jar:lib/*:conf org.apache.zookeeper.server.quorum.QuorumPeerMain zoo.conf
```

QuorumPeerMain starts a ZooKeeper server, link:http://java.sun.com/javase/technologies/core/mntr-mgmt/javamanagement/[JMX] management beans are also registered which allows management through a JMX management console.
The link:https://zookeeper.apache.org/doc/r3.5.8/zookeeperJMX.html[ZooKeeper JMX document] contains details on managing ZooKeeper with JMX.
See the script bin/zkServer.sh, which is included in the release, for an example of starting server instances.

. Test your deployment by connecting to the hosts: In Java, you can run the following command to execute simple operations: +
+
```
$ bin/zkCli.sh -server 127.0.0.1:2181
```

##### Configuration Parameters

**Minimum Configuration**

* **clientPort** : the port to listen for client connections; that is, the port that clients attempt to connect to.
* **secureClientPort** : the port to listen on for secure client connections using SSL. clientPort specifies the port for plaintext connections while secureClientPort specifies the port for SSL connections. Specifying both enables mixed-mode while omitting either will disable that mode. Note that SSL feature will be enabled when user plugs-in zookeeper.serverCnxnFactory, zookeeper.clientCnxnSocket as Netty.
* **dataDir** : the location where ZooKeeper will store the in-memory database snapshots and, unless specified otherwise, the transaction log of updates to the database. +
+
[NOTE]
====
Be careful where you put the transaction log. A dedicated transaction log device is key to consistent good performance. Putting the log on a busy device will adversely effect performance.
====
* **tickTime** : the length of a single tick, which is the basic time unit used by ZooKeeper, as measured in milliseconds. It is used to regulate heartbeats, and timeouts. For example, the minimum session timeout will be two ticks.

**Advanced Configuration**

* **initLimit** : (No Java system property) Amount of time, in ticks (see tickTime), to allow followers to connect and sync to a leader. Increased this value as needed, if the amount of data managed by ZooKeeper is large.
* **syncLimit** : (No Java system property) Amount of time, in ticks (see tickTime), to allow followers to sync with ZooKeeper. If followers fall too far behind a leader, they will be dropped.
* **server.x**=[hostname]:nnnnn[:nnnnn], etc : (No Java system property) servers making up the ZooKeeper ensemble.
When the server starts up, it determines which server it is by looking for the file myid in the data directory.
That file contains the server number, in ASCII, and it should match **x** in **server.x** in the left hand side of this setting.
The list of servers that make up ZooKeeper servers that is used by the clients must match the list of ZooKeeper servers that each ZooKeeper server has.
There are two port numbers **nnnnn**.
The first followers use to connect to the leader, and the second is for leader election.
If you want to test multiple servers on a single machine, then different ports can be used for each server.